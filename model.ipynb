{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains two PyTorch modules which together consist of the SEGAN model architecture\n",
    "(based on the paper: Pascual et al. https://arxiv.org/pdf/1703.09452.pdf).\n",
    "Modification of the initialization parameters allows the change of the model described in the class project,\n",
    "such as turning the generator to a VAE, or removing the latent variable concatenation.\n",
    "\n",
    "Loss functions for training SEGAN are also defined in this file.\n",
    "\n",
    "Authors\n",
    " * Francis Carter 2021\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from math import floor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    \"\"\"CNN Autoencoder model to clean speech signals.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    kernel_size : int\n",
    "        Size of the convolutional kernel.\n",
    "    latent_vae : bool\n",
    "        Whether or not to convert the autoencoder to a vae\n",
    "    z_prob : bool\n",
    "        Whether to remove the latent variable concatenation. Is only applicable if latent_vae is False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, latent_vae, z_prob):\n",
    "        super().__init__()\n",
    "        self.EncodeLayers = torch.nn.ModuleList()\n",
    "        self.DecodeLayers = torch.nn.ModuleList()\n",
    "        self.kernel_size = 5\n",
    "        self.latent_vae = latent_vae\n",
    "        self.z_prob = z_prob\n",
    "        EncoderChannels = [1, 16, 32, 32, 64, 64, 128, 128, 256, 256, 512, 1024]\n",
    "        DecoderChannels = [2048, 1024, 512, 512, 256, 256, 128, 128, 64, 64, 32, 1]\n",
    "\n",
    "        # Create encoder and decoder layers.\n",
    "        for i in range(len(EncoderChannels) - 1):\n",
    "            if i == len(EncoderChannels) - 2 and self.latent_vae:\n",
    "                outs = EncoderChannels[i + 1] * 2\n",
    "            else:\n",
    "                outs = EncoderChannels[i + 1]\n",
    "            self.EncodeLayers.append(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=EncoderChannels[i],\n",
    "                    out_channels=outs,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=2,\n",
    "                    padding=floor(kernel_size / 2),  # same\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for i in range(len(DecoderChannels) - 1):\n",
    "            if i == 0 and self.latent_vae:\n",
    "                ins = EncoderChannels[-1 * (i + 1)]\n",
    "            else:\n",
    "                ins = EncoderChannels[-1 * (i + 1)] * 2\n",
    "            self.DecodeLayers.append(\n",
    "                nn.ConvTranspose1d(\n",
    "                    in_channels=ins,\n",
    "                    out_channels=EncoderChannels[-1 * (i + 2)],\n",
    "                    kernel_size=kernel_size + 1,  # adding one to kernel size makes the dimensions match\n",
    "                    stride=2,\n",
    "                    padding=floor(kernel_size / 2),  # same\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through autoencoder\"\"\"\n",
    "        # encode\n",
    "        skips = []\n",
    "        x = x.permute(0, 2, 1)\n",
    "        for i, layer in enumerate(self.EncodeLayers):\n",
    "            x = layer(x)\n",
    "            skips.append(x.clone())\n",
    "            if i == len(self.DecodeLayers) - 1:\n",
    "                continue\n",
    "            else:\n",
    "                x = F.leaky_relu(x, negative_slope=0.3)\n",
    "\n",
    "        # fuse z\n",
    "        if self.latent_vae:\n",
    "            z_mean, z_logvar = x.chunk(2, dim=1)\n",
    "            x = z_mean + torch.exp(z_logvar / 2.0) * torch.randn_like(\n",
    "                z_logvar, device=x.device\n",
    "            )  # sampling from latent var probability distribution\n",
    "        elif self.z_prob:\n",
    "            z = torch.normal(torch.zeros_like(x), torch.ones_like(x))\n",
    "            x = torch.cat((x, z), 1)\n",
    "        else:\n",
    "            z = torch.zeros_like(x)\n",
    "            x = torch.cat((x, z), 1)\n",
    "\n",
    "        # decode\n",
    "        for i, layer in enumerate(self.DecodeLayers):\n",
    "            x = layer(x)\n",
    "            if i == len(self.DecodeLayers) - 1:\n",
    "                continue\n",
    "            else:\n",
    "                x = torch.cat((x, skips[-1 * (i + 2)]), 1)\n",
    "                x = F.leaky_relu(x, negative_slope=0.3)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        if self.latent_vae:\n",
    "            return x, z_mean, z_logvar\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    \"\"\"CNN discriminator of SEGAN\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    kernel_size : int\n",
    "        Size of the convolutional kernel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.Layers = torch.nn.ModuleList()\n",
    "        self.Norms = torch.nn.ModuleList()\n",
    "        Channels = [2, 16, 32, 32, 64, 64, 128, 128, 256, 256, 512, 1024, 1]\n",
    "        # Create encoder and decoder layers.\n",
    "        for i in range(len(Channels) - 1):\n",
    "            if i != len(Channels) - 2:\n",
    "                self.Layers.append(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=Channels[i],\n",
    "                        out_channels=Channels[i + 1],\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=2,\n",
    "                        padding=floor(kernel_size / 2),  # same\n",
    "                    )\n",
    "                )\n",
    "                self.Norms.append(\n",
    "                    nn.BatchNorm1d(\n",
    "                        num_features=Channels[\n",
    "                            i + 1\n",
    "                        ]  # not sure what the last dim should be here\n",
    "                    )\n",
    "                )\n",
    "            # output convolution\n",
    "            else:\n",
    "                self.Layers.append(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=Channels[i],\n",
    "                        out_channels=Channels[i + 1],\n",
    "                        kernel_size=1,\n",
    "                        stride=1,\n",
    "                        padding=0,  # same\n",
    "                    )\n",
    "                )\n",
    "                self.Layers.append(\n",
    "                    nn.Linear(in_features=8, out_features=1,)  # Channels[i+1],\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass through the discriminator\"\"\"\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # encode\n",
    "        for i in range(len(self.Norms)):\n",
    "            x = self.Layers[i](x)\n",
    "            x = self.Norms[i](x)\n",
    "            x = F.leaky_relu(x, negative_slope=0.3)\n",
    "\n",
    "        # output\n",
    "        x = self.Layers[-2](x)\n",
    "        x = self.Layers[-1](x)\n",
    "        # x = F.sigmoid(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x  # in logit format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d1_loss(d_outputs, reduction=\"mean\"):\n",
    "    \"\"\"Calculates the loss of the discriminator when the inputs are clean    \"\"\"\n",
    "    output = 0.5 * ((d_outputs - 1) ** 2)\n",
    "    if reduction == \"mean\":\n",
    "        return output.mean()\n",
    "    elif reduction == \"batch\":\n",
    "        return output.view(output.size(0), -1).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2_loss(d_outputs, reduction=\"mean\"):\n",
    "    \"\"\"Calculates the loss of the discriminator when the inputs are not clean    \"\"\"\n",
    "    output = 0.5 * ((d_outputs) ** 2)\n",
    "    if reduction == \"mean\":\n",
    "        return output.mean()\n",
    "    elif reduction == \"batch\":\n",
    "        return output.view(output.size(0), -1).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g3_loss(\n",
    "    d_outputs,\n",
    "    predictions,\n",
    "    targets,\n",
    "    length,\n",
    "    l1LossCoeff,\n",
    "    klLossCoeff,\n",
    "    z_mean=None,\n",
    "    z_logvar=None,\n",
    "    reduction=\"mean\",\n",
    "):\n",
    "    \"\"\"Calculates the loss of the generator given the discriminator outputs    \"\"\"\n",
    "    discrimloss = 0.5 * ((d_outputs - 1) ** 2)\n",
    "    l1norm = torch.nn.functional.l1_loss(predictions, targets, reduction=\"none\")\n",
    "\n",
    "    if not (\n",
    "        z_mean is None\n",
    "    ):  # This will determine if model is being trained as a vae\n",
    "        ZERO = torch.zeros_like(z_mean)\n",
    "        distq = torch.distributions.normal.Normal(\n",
    "            z_mean, torch.exp(z_logvar) ** (1 / 2)\n",
    "        )\n",
    "        distp = torch.distributions.normal.Normal(\n",
    "            ZERO, torch.exp(ZERO) ** (1 / 2)\n",
    "        )\n",
    "        kl = torch.distributions.kl.kl_divergence(distq, distp)\n",
    "        kl = kl.sum(axis=1).sum(axis=1).mean()\n",
    "    else:\n",
    "        kl = 0\n",
    "    if reduction == \"mean\":\n",
    "        return (\n",
    "            discrimloss.mean() + l1LossCoeff * l1norm.mean() + klLossCoeff * kl\n",
    "        )\n",
    "    elif reduction == \"batch\":\n",
    "        dloss = discrimloss.view(discrimloss.size(0), -1).mean(1)\n",
    "        lloss = l1norm.view(l1norm.size(0), -1).mean(1)\n",
    "        return dloss + l1LossCoeff * lloss + klLossCoeff * kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, clean_folder, noise_folder):\n",
    "        self.clean_folder = clean_folder\n",
    "        self.noise_folder = noise_folder\n",
    "\n",
    "        self.clean_files = os.listdir(clean_folder)\n",
    "        self.noise_files = os.listdir(noise_folder)\n",
    "\n",
    "        self.clean_files.sort()\n",
    "        self.noise_files.sort()\n",
    "\n",
    "        self.segments = []\n",
    "        self.size = 203905 # precomputed\n",
    "\n",
    "        self.next_idx = 0\n",
    "        self.window_size = 16384\n",
    "        self.hop_length = self.window_size//2\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.size:\n",
    "            return self.size\n",
    "        \n",
    "        self.size = 0\n",
    "        for f in self.clean_files:\n",
    "            clean_path = os.path.join(self.clean_folder, f)\n",
    "            clean_waveform, _ = torchaudio.load(clean_path, normalize=True)\n",
    "            \n",
    "            start_idx = 0\n",
    "            while start_idx < clean_waveform.size(1):\n",
    "                self.size += 1\n",
    "                start_idx += self.hop_length\n",
    "            \n",
    "        return self.size\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if i < len(self.segments):\n",
    "            return self.segments[i] # check memory footprint\n",
    "        \n",
    "        clean_path = os.path.join(self.clean_folder, self.clean_files[self.next_idx])\n",
    "        noise_path = os.path.join(self.noise_folder, self.noise_files[self.next_idx])\n",
    "\n",
    "        self.next_idx += 1\n",
    "\n",
    "        clean_waveform, _ = torchaudio.load(clean_path, normalize=True)\n",
    "        noise_waveform, _ = torchaudio.load(noise_path, normalize=True)\n",
    "\n",
    "        start_idx = 0\n",
    "        while start_idx < clean_waveform.size(1):\n",
    "            clean_segment = clean_waveform[:, start_idx:start_idx + self.window_size]\n",
    "            noise_segment = noise_waveform[:, start_idx:start_idx + self.window_size]\n",
    "\n",
    "            if clean_waveform.size(1) - start_idx < self.window_size:\n",
    "                add_len = self.window_size - (clean_waveform.size(1) - start_idx)\n",
    "                clean_segment = torch.cat([clean_segment, clean_waveform[:, :add_len]], dim=1)\n",
    "                noise_segment = torch.cat([noise_segment, noise_waveform[:, :add_len]], dim=1)\n",
    "\n",
    "            self.segments.append((noise_segment.T, clean_segment.T))\n",
    "            start_idx += self.hop_length\n",
    "        \n",
    "        return self.segments[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_folder = './data/tr_clean/'\n",
    "noise_folder = './data/tr_noise/'\n",
    "\n",
    "batch_size = 100\n",
    "dataset = SpeechDataset(clean_folder, noise_folder)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(kernel_size=5, latent_vae=True, z_prob=True)\n",
    "discriminator = Discriminator(kernel_size=5)\n",
    "\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "l1LossCoeff = 1\n",
    "klLossCoeff = 1\n",
    "\n",
    "num_epochs = 10\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (noisy_speech, clean_speech) in enumerate(dataloader):\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        generated_speech, _, _ = generator(noisy_speech)\n",
    "        generated_speech = generated_speech.detach()\n",
    "\n",
    "        fake_inputs = torch.cat([generated_speech, clean_speech], dim=2)\n",
    "        d_outputs_generated = discriminator(fake_inputs)\n",
    "        \n",
    "        real_inputs = torch.cat([clean_speech, clean_speech], dim=2) # check this\n",
    "        d_outputs_real = discriminator(real_inputs)\n",
    "        \n",
    "        loss_d = d1_loss(d_outputs_real) + d2_loss(d_outputs_generated)\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        denoised_speech, mu, logvar = generator(noisy_speech)\n",
    "        d_inputs_fake = torch.cat([denoised_speech, clean_speech], dim=2)\n",
    "        d_outputs_fake = discriminator(d_inputs_fake)\n",
    "        loss_g = g3_loss(d_outputs_fake, denoised_speech, clean_speech, noisy_speech.size(1), l1LossCoeff, klLossCoeff, z_mean=mu, z_logvar=logvar)\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # Print training information\n",
    "        if i % log_interval == 0:\n",
    "            print(f\"Epoch {epoch}/{num_epochs}, Batch {i}/{len(dataloader)}, Loss D: {loss_d.item()}, Loss G: {loss_g.item()}\")\n",
    "\n",
    "# Save the trained models if needed\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
